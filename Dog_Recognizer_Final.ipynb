{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41de2965-6abd-4d87-9399-c0915140feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c47c559-f31c-477c-bc5a-70ed761eef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "318fde56-8537-4921-a0a8-9c66911db80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb6c999-0363-42b7-b2a5-6849494d1604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mAnnotation\u001b[m\u001b[m/            annotation.tar         train_data.mat\n",
      "\u001b[34mAnnotation 2\u001b[m\u001b[m/          lists.tar              vae_mnist_interp.webp\n",
      "\u001b[34mImages\u001b[m\u001b[m/                test_data.mat\n"
     ]
    }
   ],
   "source": [
    "ls '/Users/michaelmoore/Desktop/dog_essay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d86cc556-7ed3-4c97-b269-a88804c9e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = r\"/Users/michaelmoore/Desktop/dog_essay/images\"\n",
    "data_path =  r\"/Users/michaelmoore/Desktop/dog_essay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e66a2cb7-b41f-497c-95ce-ed24c2cf79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8aa3d5f-2a61-4482-b40a-e312c3eda2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4450f37-f547-4442-9cd7-286f31ab24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e604a534-1ee2-450a-b69e-61d86229cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e32c45d-c3d9-49d0-bcb7-c26316f175dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31caf910-23c1-42f9-b01c-fee2f1a4d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e055857-97da-4ce6-8b59-a52e9dbc6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e6310-f3c9-445a-902b-2745de7f0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "train_dict = scipy.io.loadmat('/Users/michaelmoore/Desktop/dog_essay/train_data.mat')\n",
    "test_dict = scipy.io.loadmat('/Users/michaelmoore/Desktop/dog_essay/test_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d17087-1858-42ec-8a04-651037d47e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9bb5ee-32fc-4a41-baf2-4ebece9679bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"/Users/michaelmoore/Desktop/dog_essay\"))\n",
    "#['all-dogs']\n",
    "\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "PATH = '/Users/michaelmoore/Desktop/dog_essay/images'\n",
    "images = os.listdir(PATH)\n",
    "print(f'There are {len(os.listdir(PATH))} pictures of dogs.')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\n",
    "\n",
    "for indx, axis in enumerate(axes.flatten()):\n",
    "    try:\n",
    "        rnd_indx = np.random.randint(0, len(os.listdir(PATH)))\n",
    "    # https://matplotlib.org/users/image_tutorial.html\n",
    "        img = plt.imread(PATH + images[rnd_indx])\n",
    "        imgplot = axis.imshow(img)\n",
    "        axis.set_title(images[rnd_indx])\n",
    "        axis.set_axis_off()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "#plt.tightcreate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302ef9a-47f5-4e63-9776-033fd363bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it looks like a bunch of white dogs in a snowstorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec62a9d-d4da-49ac-968b-d32108a9ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d2716-2a2e-4194-a7b2-10931d768382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b14fea-059a-4b61-960f-b5a43f4de488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(torch.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009c618-9451-4890-b8e0-c9e560d4a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cls, pil_img, scale):\n",
    "        w, h = pil_img.size\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
    "        pil_img = pil_img.resize((newW, newH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57875e-961a-4224-9ec3-20ded4bedd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(10),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.35, 0.35, 0.406], [0.30, 0.34, 0.35])\n",
    "# ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4340123-8eb4-4a9b-9d1f-0dad3b0410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder('/Users/michaelmoore/Desktop/dog_essay/images/', transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_data, shuffle=True,\n",
    "                                           batch_size=10)\n",
    "\n",
    "imgs, label = next(iter(dataloader))\n",
    "imgs = imgs.numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b8c8e-8675-469f-a021-0e3e46d3c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader as DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0fd64-7b47-4854-a838-ff5a5aab4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "# dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=utils.collate_fn\n",
    "# )\n",
    "\n",
    "# # For Training\n",
    "# images, targets = next(iter(data_loader))\n",
    "# images = list(image for image in images)\n",
    "# targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "# output = model(images, targets)  # Returns losses and detections\n",
    "# print(output)\n",
    "\n",
    "# # For inference\n",
    "# model.eval()\n",
    "# x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# predictions = model(x)  # Returns predictions\n",
    "# print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c55bb7-31c1-447f-a535-ff6836c7f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9359c0-c913-41ee-81b9-da9dcde7ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bed557-92d0-476d-a3c1-667998789320",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd304d-a4f3-4669-bf86-119f7dc431c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import tarfile\n",
    "import os\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2880e40-fbd7-41ab-938e-afae60f144dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a3482-0bdd-4b41-b26a-9a1737dce81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea6597-689b-499a-a9af-f4e662391ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "# from layers import BBB_Linear, BBB_Conv2d\n",
    "# from layers import BBB_LRT_Linear, BBB_LRT_Conv2d\n",
    "# from layers import FlattenLayer, ModuleWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f44b8-fe8e-4c3c-85d7-0aeee4260a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\users\\michaelmoore\\Desktop\\dog_essay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063106c-47ae-472a-9b35-7af7a255a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(path)\n",
    "fname = os.listdir(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afb610-bfe1-4d80-8eb0-3ca8c023866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fname.endswith('tar.gz'):\n",
    "#     tar = tarfile.open(fname, 'r:gz')\n",
    "#     tar.extractall()\n",
    "#     tar.close()\n",
    "# elif fname.endswith(\"tar\"):\n",
    "#     tar = tarfile.open(fname, \"r:\")\n",
    "#     tar.extractall()\n",
    "#     tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20da8dc4-b109-4591-b5d0-5b5cd5230ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db49168-ab32-495c-b472-4b18b7498f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcee6c-99c2-4b7c-9930-7ab5c5742921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder (VAE) class.\n",
    "    \n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input data.\n",
    "        hidden_dim (int): Dimensionality of the hidden layer.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 8),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 8, 2 * latent_dim), # 2 for mean and variance.\n",
    "        )\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 8),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 8, hidden_dim // 4),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 2),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.SiLU(),  # Swish activation function\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Encodes the input data into the latent space.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            eps (float): Small value to avoid numerical instability.\n",
    "        \n",
    "        Returns:\n",
    "            torch.distributions.MultivariateNormal: Normal distribution of the encoded data.\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        \n",
    "        return torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        \n",
    "    def reparameterize(self, dist):\n",
    "        \"\"\"\n",
    "        Reparameterizes the encoded data to sample from the latent space.\n",
    "        \n",
    "        Args:\n",
    "            dist (torch.distributions.MultivariateNormal): Normal distribution of the encoded data.\n",
    "        Returns:\n",
    "            torch.Tensor: Sampled data from the latent space.\n",
    "        \"\"\"\n",
    "        return dist.rsample()\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decodes the data from the latent space to the original input space.\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Data in the latent space.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed data in the original input space.\n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, compute_loss: bool = True):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input data.\n",
    "            compute_loss (bool): Whether to compute the loss or not.\n",
    "        \n",
    "        Returns:\n",
    "            VAEOutput: VAE output dataclass.\n",
    "        \"\"\"\n",
    "        dist = self.encode(x)\n",
    "        z = self.reparameterize(dist)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        if not compute_loss:\n",
    "            return VAEOutput(\n",
    "                z_dist=dist,\n",
    "                z_sample=z,\n",
    "                x_recon=recon_x,\n",
    "                loss=None,\n",
    "                loss_recon=None,\n",
    "                loss_kl=None,\n",
    "            )\n",
    "        \n",
    "        # compute loss terms \n",
    "        loss_recon = F.binary_cross_entropy(recon_x, x + 0.5, reduction='none').sum(-1).mean()\n",
    "        std_normal = torch.distributions.MultivariateNormal(\n",
    "            torch.zeros_like(z, device=z.device),\n",
    "            scale_tril=torch.eye(z.shape[-1], device=z.device).unsqueeze(0).expand(z.shape[0], -1, -1),\n",
    "        )\n",
    "        loss_kl = torch.distributions.kl.kl_divergence(dist, std_normal).mean()\n",
    "                \n",
    "        loss = loss_recon + loss_kl\n",
    "        \n",
    "        return VAEOutput(\n",
    "            z_dist=dist,\n",
    "            z_sample=z,\n",
    "            x_recon=recon_x,\n",
    "            loss=loss,\n",
    "            loss_recon=loss_recon,\n",
    "            loss_kl=loss_kl,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad2198-3eec-4a87-be39-310f567be153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@dataclass\n",
    "class VAEOutput:\n",
    "    \"\"\"\n",
    "    Dataclass for VAE output.\n",
    "    \n",
    "    Attributes:\n",
    "        z_dist (torch.distributions.Distribution): The distribution of the latent variable z.\n",
    "        z_sample (torch.Tensor): The sampled value of the latent variable z.\n",
    "        x_recon (torch.Tensor): The reconstructed output from the VAE.\n",
    "        loss (torch.Tensor): The overall loss of the VAE.\n",
    "        loss_recon (torch.Tensor): The reconstruction loss component of the VAE loss.\n",
    "        loss_kl (torch.Tensor): The KL divergence component of the VAE loss.\n",
    "    \"\"\"\n",
    "    z_dist: torch.distributions.Distribution\n",
    "    z_sample: torch.Tensor\n",
    "    x_recon: torch.Tensor\n",
    "    \n",
    "    loss: torch.Tensor\n",
    "    loss_recon: torch.Tensor\n",
    "    loss_kl: torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252b30e-d0b1-4660-81c4-778ccf1b91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "# transform = v2.Compose([\n",
    "#     v2.ToImage(), \n",
    "#     v2.ToDtype(torch.float32, scale=True),\n",
    "#     v2.Lambda(lambda x: x.view(-1) - 0.5),\n",
    "# ])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(root= data_path, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(root= data_path, transform=transform)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n",
    "# self.transform = transforms.Compose([transforms.ToTensor()])  # you can add to the list all the transformations you need. \n",
    "# return self.transform(self.x_data[index]), self.transform(self.y_data[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bf2a7c-29bb-40c3-9fa4-1f3f3855b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-2\n",
    "num_epochs = 50\n",
    "latent_dim = 2\n",
    "hidden_dim = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VAE(input_dim=784, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "#writer = SummaryWriter(f'runs/mnist/vae_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9db193-d189-457f-9f40-08fe9adfb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, writer=None):\n",
    "    \"\"\"\n",
    "    Trains the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        loss_fn: The loss function.\n",
    "        optimizer: The optimizer.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        output = model(data)  # Forward pass\n",
    "        loss = output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            # Calculate and log gradient norms\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} (Recon: {output.loss_recon.item():.4f}, KL: {output.loss_kl.item():.4f}) Grad: {total_norm:.4f}')\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/BCE', output.loss_recon.item(), global_step)\n",
    "                writer.add_scalar('Loss/Train/KLD', output.loss_kl.item(), global_step)\n",
    "                writer.add_scalar('GradNorm/Train', total_norm, global_step)\n",
    "            \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  # Update the model parameters\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d4734-d3f9-4c28-ba8d-60d7439fb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, writer=None):\n",
    "    \"\"\"\n",
    "    Tests the model on the given data.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataloader (torch.utils.data.DataLoader): The data loader.\n",
    "        cur_step (int): The current step.\n",
    "        writer: The TensorBoard writer.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #for data, target in enumerate(dataloader):\n",
    "        #for data, target in dataloader.items():\n",
    "        for data, target in enumerate(testloader):\n",
    "            data = data.to(device)\n",
    "            data = data.view(data.size(0), -1)  # Flatten the data\n",
    "            \n",
    "            output = model(data, compute_loss=True)  # Forward pass\n",
    "            \n",
    "            test_loss += output.loss.item()\n",
    "            test_recon_loss += output.loss_recon.item()\n",
    "            test_kl_loss += output.loss_kl.item()\n",
    "            \n",
    "    test_loss /= len(dataloader)\n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    print(f'====> Test set loss: {test_loss:.4f} (BCE: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f})')\n",
    "    \n",
    "    # if writer is not None:\n",
    "    #     writer.add_scalar('Loss/Test', test_loss, global_step=cur_step)\n",
    "    #     writer.add_scalar('Loss/Test/BCE', output.loss_recon.item(), global_step=cur_step)\n",
    "    #     writer.add_scalar('Loss/Test/KLD', output.loss_kl.item(), global_step=cur_step)\n",
    "        \n",
    "    #     # Log reconstructions\n",
    "    #     writer.add_images('Test/Reconstructions', output.x_recon.view(-1, 1, 28, 28), global_step=cur_step)\n",
    "    #     writer.add_images('Test/Originals', data.view(-1, 1, 28, 28), global_step=cur_step)\n",
    "        \n",
    "    #     # Log random samples from the latent space\n",
    "    #     z = torch.randn(16, latent_dim).to(device)\n",
    "    #     samples = model.decode(z)\n",
    "    #     writer.add_images('Test/Samples', samples.view(-1, 1, 28, 28), global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026bcee2-75a9-44f6-96d5-cdba9f1d851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates) #, writer=writer)\n",
    "    test(model, test_loader, prev_updates) # writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d690d-2e4f-4273-8b5b-b112dce2bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "n = 15\n",
    "z1 = torch.linspace(-0, 1, n)\n",
    "z2 = torch.zeros_like(z1) + 2\n",
    "z = torch.stack([z1, z2], dim=-1) #.to(device)\n",
    "samples = model.decode(z)\n",
    "samples = torch.sigmoid(samples)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, ax = plt.subplots(1, n, figsize=(n, 1))\n",
    "for i in range(n):\n",
    "    ax[i].imshow(samples[i].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    \n",
    "plt.savefig('vae_mnist_interp.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95107649-19b0-4d9c-b06b-79f3f5e1c449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fcef7-402f-4d95-b15b-45e42f0d1d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c975e-3fe7-46f0-afd8-0115e3452e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190689b-09b0-41b3-84ab-68dc5f9af9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
